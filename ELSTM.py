import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import StackedRNNCells, RNN

class ElmanLSTMCell(tf.keras.layers.Layer):
    def __init__(self, units):
        super(ElmanLSTMCell, self).__init__()
        self.units = units # define units
        self.state_size = [self.units, self.units]  # [hidden_state, cell_state]
    
    def build(self, input_shape):
        input_dim = input_shape[-1]
        
        # Input weights (W_f_y, W_i_y, W_cbar_y, W_o_y) for all 4 gates
        self.W_input = self.add_weight(shape=(input_dim, 4 * self.units),
                                 initializer='glorot_uniform',
                                 name='W_input')

        # Elman hidden state weights (W_f_a, W_i_a, W_cbar_a, W_o_a) for all 4 gates
        self.W_hidden = self.add_weight(shape=(self.units, 4 * self.units),
                                 initializer='glorot_uniform',
                                 name='W_hidden')
        
        # Bias for all 4 gates
        self.b = self.add_weight(shape=(4 * self.units,),
                                 initializer='zeros',
                                 name='b')
        
        # Elman cell state weights (W_x_a) gate
        self.W_cell = self.add_weight(shape=(self.units, self.units),
                                 initializer='orthogonal',
                                 name='W_cell')
        
        # Bias for cell gate
        self.b_x = self.add_weight(shape=(self.units,),
                                 initializer='zeros',
                                 name='b_x')
    
    def call(self, inputs, states):
        # Previous hidden state and previous cell state
        a_tm1, c_tm1 = states
        
        # ERN forward function
        z = tf.matmul(inputs, self.W_input) + tf.matmul(a_tm1, self.W_hidden) + self.b
        f, i, c_bar, o = tf.split(z, num_or_size_splits=4, axis=1)
        
        f_t = tf.sigmoid(f)
        i_t = tf.sigmoid(i)
        o_t = tf.sigmoid(o)
        c_bar_t = tf.tanh(c_bar)
        
        c_t = f_t * c_tm1 + i_t * c_bar_t
        a_t = o_t * tf.tanh(c_t)
        x_bar_t = tf.matmul(a_t, self.W_cell) + self.b_x
        
        return x_bar_t, [a_t, c_t]

# Dataset maker for training, validation and testing
def make_dataset(X, Y, batch_size=1, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices((X, Y))
    if shuffle:
        ds = ds.shuffle(1024)
    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Load the .npz file
data = np.load('ks_data.npz') # the data file generated by ks or lorenz
X_data = data['X_data'].astype(np.float32)  # shape (num_records, seq_length, dim_x)
Y_data = data['Y_data'].astype(np.float32)  # shape (num_records, seq_length, dim_y)
data_len  = X_data.shape[0] # entire data length
train_size  = int(data_len * 0.8) # train:val:test = 8:1:1, train set with 80% percentage size
val_size    = (data_len - train_size) // 2 # val set with 10% percentage size
test_size   = data_len - train_size - val_size # test set with 10% percentage size
X_train, Y_train = X_data[:train_size], Y_data[:train_size] # split into train dataset
X_val,   Y_val   = X_data[train_size:train_size+val_size], Y_data[train_size:train_size+val_size] # split into val dataset
X_test,  Y_test  = X_data[train_size+val_size:], Y_data[train_size+val_size:] # split into test dataset
batch_size = Y_train.shape[0] # Y as inputs with its shape size
T = Y_data.shape[1]
train_dataset = make_dataset(X_train, Y_train, batch_size, shuffle=True) # shuffle while training
val_dataset   = make_dataset(X_val,   Y_val,   batch_size, shuffle=False)
test_dataset  = make_dataset(X_test,  Y_test,  batch_size, shuffle=False)

# Build model
units = 16
input_dim = Y_data.shape[-1]
dim_y = X_data.shape[-1]
single_cell = ElmanLSTMCell(units)
mutiple_cells = [
    ElmanLSTMCell(units),
    ElmanLSTMCell(units),
]
stack_cell = StackedRNNCells(mutiple_cells) # stack mutiple layer init
rnn = RNN(single_cell, 
          return_sequences=True, 
          input_shape=(None, input_dim)) # call rnn constructor
output_layer = tf.keras.layers.Dense(dim_y)

# Set up learning rate with factor and patience
init_lr = 1e-1
min_lr = 1e-2
min_delta = 1e-3
factor = 0.5
patience = 5
halt_patience = 10
optimizer = tf.keras.optimizers.legacy.Adam(init_lr)

# Training step implementation
@tf.function
def train_step(x_batch, y_batch):
    inputs = y_batch
    outputs = x_batch

    with tf.GradientTape() as tape:
        h = rnn(inputs, training=True)
        y_pred = output_layer(h)
        loss = tf.reduce_mean((outputs - y_pred)**2)

    # Backâ€‘propagate & update all trainable weights
    train_vars = single_cell.trainable_variables + output_layer.trainable_variables
    grads = tape.gradient(loss, train_vars)
    optimizer.apply_gradients(zip(grads, train_vars))

    return loss

# Validation step implementation
@tf.function
def val_step(x_batch, y_batch):
    inputs = y_batch
    outputs = x_batch
    h = rnn(inputs, training=False)
    y_pred = output_layer(h)
    return tf.reduce_mean((outputs - y_pred)**2)

# Training loop
epochs = 15000
best_val = float('inf')
halt_count = 0  
wait = 0

for epoch in range(1, epochs+1):  
    # Train
    total_train_loss = 0.0
    train_steps = 0
    for x_batch, y_batch in train_dataset:
        batch_loss = train_step(x_batch, y_batch)
        total_train_loss += batch_loss
        train_steps += 1
    train_loss = total_train_loss / train_steps

    # Validate
    total_val_loss = 0.0
    val_steps = 0
    for x_batch, y_batch in val_dataset:
        val_loss = val_step(x_batch, y_batch)
        total_val_loss += val_loss
        val_steps += 1
    val_loss = total_val_loss / val_steps
    
    if val_loss > best_val:
        halt_count += 1
    else:
        halt_count = 0
    
    if halt_count >= halt_patience:
        print(f"No more effective epoch at {epoch:03d}: Train MSE = {train_loss:.4f}, Val MSE = {val_loss:.4f}")
        break
    
    # Apply new learning rate
    if val_loss < best_val - min_delta:
        best_val = val_loss
        wait = 0
        halt_count = 0
    else:
        wait += 1
        if wait >= patience:
            # decay the LR
            new_lr = max(optimizer.learning_rate.numpy() * factor, min_lr)
            optimizer.learning_rate.assign(new_lr)
            wait = 0

    print(f"Epoch {epoch:03d}: Train MSE = {train_loss:.4f}, Val MSE = {val_loss:.4f} (LR={optimizer.learning_rate.numpy():.4f})")

# Eval loop
total_test_loss = 0.0
test_steps = 0
for x_batch, y_batch in test_dataset:
    t_loss = val_step(x_batch, y_batch)
    total_test_loss += t_loss
    test_steps += 1
print(f"Test  MSE = {total_test_loss / test_steps:.4f}")