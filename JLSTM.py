import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
from tensorflow.keras.layers import StackedRNNCells, RNN

class JordanLSTMCell(tf.keras.layers.Layer):
    def __init__(self, input_size, hidden_size, output_size):
        super(JordanLSTMCell, self).__init__()
        self.input_size = input_size  # define input size
        self.hidden_size = hidden_size # define hidden size
        self.output_size = output_size # define output size
        self.state_size = [self.output_size, self.hidden_size]  # [hidden_state, cell_state]
    
    def build(self, input_shape):
        # Jordan estimated state weights (W_f_x, W_i_x, W_cbar_x, W_o_x) for all 4 gates
        self.W_estimate = self.add_weight(shape=(self.output_size, 4 * self.hidden_size),
                                 initializer='glorot_uniform',
                                 name='W_estimate')
        
        # Input weights (W_f_y, W_i_y, W_cbar_y, W_o_y) for all 4 gates
        self.W_input = self.add_weight(shape=(self.input_size, 4 * self.hidden_size),
                                 initializer='glorot_uniform',
                                 name='W_input')
        
        # Bias for all 4 gates
        self.b = self.add_weight(shape=(4 * self.hidden_size,),
                                 initializer='zeros',
                                 name='b')
        
        # Elman cell state weights (W_x_a) gate
        self.W_cell = self.add_weight(shape=(self.hidden_size, self.output_size),
                                 initializer='orthogonal',
                                 name='W_cell')
        
        # Bias for cell gate
        self.b_x = self.add_weight(shape=(self.output_size,),
                                 initializer='zeros',
                                 name='b_x')
        
        super().build(self.input_size)
    
    def call(self, inputs, states):
        # Previous estimated state and previous cell state
        x_bar_tm1, c_tm1 = states
        
        # JRN forward function
        z = tf.matmul(inputs, self.W_input) + tf.matmul(x_bar_tm1, self.W_estimate) + self.b
        f, i, c_bar, o = tf.split(z, num_or_size_splits=4, axis=1)
        
        f_t = tf.sigmoid(f)
        i_t = tf.sigmoid(i)
        o_t = tf.sigmoid(o)
        c_bar_t = tf.tanh(c_bar)
        
        c_t = f_t * c_tm1 + i_t * c_bar_t
        a_t = o_t * tf.tanh(c_t)
        x_bar_t = tf.matmul(a_t, self.W_cell) + self.b_x
        output = x_bar_t
        new_states = [x_bar_t, c_t]
        
        return output, new_states

# Dataset maker for training, validation and testing
def make_dataset(X, Y, batch_size=1, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices((X, Y))
    if shuffle:
        ds = ds.shuffle(1024)
    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Load the .npz file
data = np.load('lorenz_data.npz') # the data file generated by ks or lorenz
X_data = data['X_data'].astype(np.float32)   # shape (num_records, seq_length, dim_x)
Y_data = data['Y_data'].astype(np.float32)   # shape (num_records, seq_length, dim_y)
data_len  = 50
train_size  = int(data_len * 0.8) # train:val:test = 8:1:1, train set with 80% percentage size
val_size    = (data_len - train_size) // 2 # val set with 10% percentage size
test_size   = data_len - train_size - val_size # test set with 10% percentage size

X_train, Y_train = X_data[:train_size], Y_data[:train_size] # split into train dataset
X_val,   Y_val   = X_data[train_size:train_size + val_size], Y_data[train_size:train_size + val_size] # split into val dataset
X_test,  Y_test  = X_data[train_size + val_size:], Y_data[train_size + val_size:] # split into test dataset
X_train_reshaped = X_train.reshape(-1,X_train.shape[2])
Y_train_reshaped = Y_train.reshape(-1,Y_train.shape[2])

mean_x = X_train_reshaped.mean(axis=0)
std_x = X_train_reshaped.std(axis=0)
mean_y = Y_train_reshaped.mean(axis=0)
std_y = Y_train_reshaped.std(axis=0)
print("mean of states is",mean_x)
print("mean of measurements is",mean_y)
print("standard deviation of states is",std_x)
print("standard deviation of measurements is",std_y)

X_train_norm = (X_train - mean_x) / std_x
Y_train_norm = (Y_train - mean_y) / std_y
X_val_norm = (X_val - mean_x) / std_x
Y_val_norm = (Y_val - mean_y) / std_y
X_test_norm = (X_test - mean_x)/std_x
Y_test_norm = (Y_test - mean_y)/std_y

batch_size = 5
train_dataset = make_dataset(X_train_norm, Y_train_norm, batch_size, shuffle=False)
val_dataset   = make_dataset(X_val_norm,   Y_val_norm,   batch_size, shuffle=False)
test_dataset  = make_dataset(X_test_norm,  Y_test_norm,  batch_size, shuffle=False)

# Build model
dim_y = 1
dim_x = 3
hidden_size = 50
single_cell = JordanLSTMCell(dim_y, hidden_size, dim_x)
mutiple_cells = [
    JordanLSTMCell(dim_y, hidden_size, dim_x),
    JordanLSTMCell(dim_x, hidden_size, dim_x),
]
stack_cell = StackedRNNCells(mutiple_cells) # stack mutiple layer init
rnn = RNN(single_cell, 
          return_sequences=True, 
          input_shape=(None, dim_y)) # call rnn constructor
criterion = tf.keras.losses.MeanSquaredError() # Use Mean Squared Error as loss function

# Hyperparameters for composite loss function
alpha = 0.1
H = tf.constant([[1.0],     # x-component weight
                 [1.0],     # y-component weight
                 [0.0]],    # z-component weight
                dtype=tf.float32)       # shape (dim_x, dim_y)

# Set up learning rate with factor and patience
init_lr = 1e-1
min_lr = 1e-2
min_delta = 1e-3
factor = 0.5
patience = 5
halt_patience = 10
optimizer = tf.keras.optimizers.legacy.Adam(init_lr)

# Custom composite loss function
@tf.function
def composite_loss(x_true, x_pred, y_true, alpha: float = alpha):
    mse_states = tf.reduce_mean(tf.square(x_true - x_pred))
    y_pred = tf.einsum('...i,ij->...j', x_pred, H) 
    mse_inputs = tf.reduce_mean(tf.square(y_true - y_pred))
    return alpha * mse_states + (1.0 - alpha) * mse_inputs

# Training step implementation
@tf.function
def train_step(x_batch, y_batch):
    input_data = y_batch
    target_x = x_batch

    with tf.GradientTape() as tape:
        pred_x = rnn(input_data, training=True)
        loss = composite_loss(target_x, pred_x, input_data)
    
    # Backâ€‘propagate & update all trainable weights
    train_vars = rnn.trainable_variables
    grads = tape.gradient(loss, train_vars)
    optimizer.apply_gradients(zip(grads, train_vars))

    return loss

# Validation step implementation
@tf.function
def val_step(x_batch, y_batch):
    input_data = y_batch
    target_x = x_batch
    pred_x = rnn(input_data, training=False)
    loss = composite_loss(target_x, pred_x, input_data)
    return loss

# Training loop
epochs = 50
best_val = float('inf')
halt_count = 0  
wait = 0
start_train = time.time()
val_losses = []

for epoch in range(1, epochs + 1):
    # Train
    total_train_loss = 0.0
    train_steps = 0
    for x_batch, y_batch in train_dataset:
        batch_loss = train_step(x_batch, y_batch)
        total_train_loss += batch_loss
        train_steps += 1
    train_loss = total_train_loss / train_steps

    # Validate
    total_val_loss = 0.0
    val_steps = 0
    for x_batch, y_batch in val_dataset:
        val_loss = val_step(x_batch, y_batch)
        total_val_loss += val_loss
        val_steps += 1
    val_loss = total_val_loss / val_steps
    
    val_losses.append(val_loss.numpy() if isinstance(val_loss, tf.Tensor) else val_loss)
    
    if val_loss >= best_val:
        halt_count += 1
    else:
        best_val = val_loss
        halt_count = 0
    
    if halt_count >= halt_patience:
        print(f"No more effective epoch at {epoch:03d}: Train MSE = {train_loss:.4f}, Val MSE = {val_loss:.4f}")
        break
    
    # Apply new learning rate
    if val_loss < best_val - min_delta:
        best_val = val_loss
        wait = 0
        halt_count = 0
    else:
        wait += 1
        if wait >= patience:
            # decay the LR
            new_lr = max(optimizer.learning_rate.numpy() * factor, min_lr)
            optimizer.learning_rate.assign(new_lr)
            wait = 0

    print(f"Epoch {epoch:03d}: Train MSE = {train_loss:.4f}, Val MSE = {val_loss:.4f} (LR={optimizer.learning_rate.numpy():.4f})")

end_train = time.time()
print("time taken to train JLSTM:",end_train-start_train)

# Eval loop
start_eval = time.time()
total_test_loss = 0.0
test_steps = 0
for x_batch, y_batch in test_dataset:
    t_loss = val_step(x_batch, y_batch)
    total_test_loss += t_loss
    test_steps += 1
print(f"Test  MSE = {total_test_loss / test_steps:.4f}")
end_eval = time.time()
print("Time taken to test JLSTM:", end_eval - start_eval)

plt.figure()
epochs_list = list(range(1, len(val_losses) + 1))
plt.plot(epochs_list, val_losses, marker='o')
plt.title('Validation Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.grid(True)
plt.tight_layout()
plt.show()
